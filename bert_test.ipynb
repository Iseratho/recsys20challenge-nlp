{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "8g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "14g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# bert test\n",
        "\n",
        "This is a text cell. Start editing!"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592209543199,
          "endTs" : 1592209543296
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer}\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "import scala.collection.mutable.WrappedArray\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592207344682,
          "endTs" : 1592207725652
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val bert = BertEmbeddings.pretrained(name=\"bert_multi_cased\", lang=\"xx\")"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "bert_multi_cased download started this may take some time.\n",
            "Approximate size to download 638.7 MB\n",
            "Download done! Loading the resource.\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592207725656,
          "endTs" : 1592207725929
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sentenceStartTokenId = 101\n",
        "val sentenceEndTokenId = 102"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592207725933,
          "endTs" : 1592207726898
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val docs = Seq(Array(123,119, 11025), Array(10135))\n",
        "val prep = docs.map(tokens => {\n",
        "    Array(sentenceStartTokenId) ++\n",
        "      tokens ++\n",
        "      Array(sentenceEndTokenId) ++\n",
        "      Array.fill(bert.getMaxSentenceLength - tokens.length - 2)(0)\n",
        "})\n",
        "prep"
      ],
      "outputs" : [
        {
          "execution_count" : 5,
          "data" : {
            "text/plain" : [
              "List([I@4d56e7bd, [I@be3b8ba)"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Seq[Array[Int]]"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592207921792,
          "endTs" : 1592207922541
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val tfBert = bert.getModelIfNotSet\n",
        "tfBert"
      ],
      "outputs" : [
        {
          "execution_count" : 8,
          "data" : {
            "text/plain" : [
              "com.johnsnowlabs.ml.tensorflow.TensorflowBert@51f6faa2"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "TensorflowBert"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592207979436,
          "endTs" : 1592207979829
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// Code to use embeddings of bert\n",
        "tfBert.tag(prep, \"module/bert/encoder/Reshape_1:0\", bert.getMaxSentenceLength)"
      ],
      "outputs" : [
        {
          "execution_count" : 4,
          "data" : {
            "text/plain" : [
              "List([[F@5d54059d, [[F@4104bc2)"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Seq[Array[Array[Float]]]"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592208296716,
          "endTs" : 1592208297042
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val trainDf = Seq(0 to 100:_*).toDF()\n",
        "    .withColumn(\"tokens\", when(rand() > 0.2, Array(101, 123,119, 11025, 102)).otherwise(Array(101, 10135, 102)))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592209765695,
          "endTs" : 1592209769833
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// val average_embeddings = \n",
        "val udf_aggBert = udf[Array[Float], WrappedArray[Int]](tokens => {\n",
        "            val input = Seq(tokens.toArray ++ Array.fill(bert.getMaxSentenceLength - tokens.length)(0))\n",
        "            val embeddings = tfBert.tag(input, \"module/bert/encoder/Reshape_1:0\", bert.getMaxSentenceLength)\n",
        "            val sumEmbeddings = embeddings.head.transpose.map(_.sum)\n",
        "            sumEmbeddings.map(x => (x * 1.0 / bert.getMaxSentenceLength).toFloat)\n",
        "})\n",
        "trainDf.withColumn(\"embeddings\", udf_aggBert('tokens)).show"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+--------------------+--------------------+\n",
            "|value|              tokens|          embeddings|\n",
            "+-----+--------------------+--------------------+\n",
            "|    0|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    1|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    2|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    3|   [101, 10135, 102]|[0.71374446, 0.42...|\n",
            "|    4|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    5|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    6|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    7|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    8|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|    9|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   10|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   11|   [101, 10135, 102]|[0.71374446, 0.42...|\n",
            "|   12|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   13|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   14|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   15|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   16|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   17|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "|   18|   [101, 10135, 102]|[0.71374446, 0.42...|\n",
            "|   19|[101, 123, 119, 1...|[0.69951665, 0.43...|\n",
            "+-----+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}