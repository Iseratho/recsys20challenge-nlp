{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "12g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "2g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# NLP RECSYSÂ pipeline test\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592175745079,
          "endTs" : 1592175745618
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import com.johnsnowlabs.ml.tensorflow.TensorflowBert\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "\n",
        "// Imports below are used for customer ML Estimator\n",
        "import org.apache.spark.ml.Estimator\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "\n",
        "val dataDir = sys.env(\"HOME\") + \"/recsys2020\""
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592176274722
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "var trainDf = Seq(0 to 1000000:_*).toDF()\n",
        "    .withColumn(\"tweet_id\", monotonically_increasing_id)\n",
        "    .withColumn(\"user_id\", monotonically_increasing_id)\n",
        "for (i <- 0 to 522)\n",
        "{\n",
        "    trainDf = trainDf.withColumn(\"f_\" + i, rand)\n",
        "}\n",
        "   \n",
        "trainDf.show()\n",
        "trainDf.printSchema"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 19,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592176069901,
          "endTs" : 1592176070409
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "trainDf.count()"
      ],
      "outputs" : [
        {
          "execution_count" : 19,
          "data" : {
            "text/plain" : [
              "100"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Long"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592161272825,
          "endTs" : 1592161272932
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// NLP Feature Engineering\n",
        "\n",
        "// val doc = new DocumentAssembler()\n",
        "//     .setInputCol(\"text\")\n",
        "//     .setOutputCol(\"document\")\n",
        "//     .setCleanupMode(\"shrink\")\n",
        "// val tok = new Tokenizer()\n",
        "//     .setInputCols(\"document\")\n",
        "//     .setOutputCol(\"token\")\n",
        "//     .setContextChars(Array(\"(\", \")\", \"?\", \"!\"))\n",
        "//     .setSplitChars(Array(\"-\"))\n",
        "//     .addException(\"New York\")\n",
        "//     .addException(\"e-mail\")\n",
        "// val bert = BertEmbeddings.pretrained(name=\"bert_multi_cased\", lang=\"xx\")\n",
        "//       .setInputCols(\"document\", \"token\")\n",
        "//       .setOutputCol(\"embeddings\")\n",
        "//       .setPoolingLayer(0) // 0, -1, or -2\n",
        "// val use = UniversalSentenceEncoder\n",
        "//       .pretrained()\n",
        "//       .setInputCols(Array(\"document\"))\n",
        "//       .setOutputCol(\"tweet_embeddings\")\n",
        "// val emb = new SentenceEmbeddings()\n",
        "//       .setInputCols(Array(\"document\", \"embeddings\"))\n",
        "//       .setOutputCol(\"tweet_embeddings\")\n",
        "//       .setPoolingStrategy(\"AVERAGE\")\n",
        "val fin = new EmbeddingsFinisher()\n",
        "      .setInputCols(\"tweet_embeddings\")\n",
        "      .setOutputCols(\"nested_tweet_embeddings\")\n",
        "      .setOutputAsVector(true)\n",
        "      .setCleanAnnotations(false)\n",
        "\n",
        "// There should be only 1 tweet embedding vector per tweet\n",
        "// val toVec = new VectorAssembler()\n",
        "//   .setInputCols(Array(\"nested_tweet_embeddings\"))\n",
        "//   .setOutputCol(\"tweet_embeddings_vector\")\n",
        "// val embSlice = new VectorSlicer()\n",
        "//       .setInputCol(\"nested_tweet_embeddings\")\n",
        "//       .setOutputCol(\"finished_tweet_embeddings\")\n",
        "//       .setIndices(Array(0))\n",
        "\n",
        "// TODO: add from below\n",
        "// Remaining feature engineering\n",
        "\n",
        "\n",
        "// Assemble the pipeline\n",
        "val text_trans_pipeline = new Pipeline().setStages(Array(doc, use, fin))//, toVec, embSlice))"
      ],
      "outputs" : [
        {
          "execution_count" : 5,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell5",
                  "start" : 1620,
                  "end" : 1623,
                  "point" : 1620
                },
                "msg" : "not found: value doc",
                "severity" : 2
              },
              {
                "pos" : {
                  "sourceId" : "Cell5",
                  "start" : 1625,
                  "end" : 1628,
                  "point" : 1625
                },
                "msg" : "not found: value use",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not found: value doc (1620)",
              "Error: not found: value use (1625)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592161272937,
          "endTs" : 1592161272971
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// can later be combined into one large pipeline\n",
        "val intermediateDf = text_trans_pipeline.fit(trainDf).transform(trainDf)\n",
        "// intermediateDf.select(\"nested_tweet_embeddings\").toDF()\n",
        "intermediateDf//.show()"
      ],
      "outputs" : [
        {
          "execution_count" : 6,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell6",
                  "start" : 70,
                  "end" : 89,
                  "point" : 70
                },
                "msg" : "not found: value text_trans_pipeline",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not found: value text_trans_pipeline (70)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592161272986,
          "endTs" : 1592161273034
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// assumption: only one embedding is generated, thus explode just flattens the list\n",
        "val transDf = intermediateDf.withColumn(\"embedding_features\", explode('nested_tweet_embeddings))\n",
        "transDf.show()"
      ],
      "outputs" : [
        {
          "execution_count" : 7,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell7",
                  "start" : 98,
                  "end" : 112,
                  "point" : 98
                },
                "msg" : "not found: value intermediateDf",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not found: value intermediateDf (98)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592161273037,
          "endTs" : 1592161273062
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "transDf"
      ],
      "outputs" : [
        {
          "execution_count" : 8,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell8",
                  "start" : 0,
                  "end" : 7,
                  "point" : 0
                },
                "msg" : "not found: value transDf",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not found: value transDf (0)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592162215951,
          "endTs" : 1592162216578
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val scaleAss = new VectorAssembler()\n",
        "  .setInputCols(Array(\"arbraryIntFeature\"))\n",
        "  .setOutputCol(\"arbraryIntFeatureVector\")\n",
        "\n",
        "val scaler = new StandardScaler()\n",
        "  .setInputCol(\"arbraryIntFeatureVector\")\n",
        "  .setOutputCol(\"scaledFeatureVector\")\n",
        "  .setWithStd(true)\n",
        "  .setWithMean(false)\n",
        "\n",
        "val ass = new VectorAssembler()\n",
        "  .setInputCols(Array(\"binaryFeature\", \"arbraryIntFeature\"))\n",
        "  .setOutputCol(\"features\")\n",
        "  .\n",
        "\n",
        "// You need to setProbabilityCol,  setPredictionCol and setRawPredictionCol\n",
        "// otherwise you get a name conflict in the pipeline\n",
        "\n",
        "val gbtT1 = new GBTClassifier()\n",
        "  .setLabelCol(\"target1\")\n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setProbabilityCol(\"prob1\")\n",
        "  .setPredictionCol(\"pred1\")\n",
        "  .setRawPredictionCol(\"rpred1\")\n",
        "  .setMaxIter(10)\n",
        "  .setFeatureSubsetStrategy(\"auto\")\n",
        "\n",
        "val gbtT2 = new GBTClassifier()\n",
        "  .setLabelCol(\"target2\")\n",
        "  .setFeaturesCol(\"features\")\n",
        "  .setProbabilityCol(\"prob2\")\n",
        "  .setPredictionCol(\"pred2\")\n",
        "  .setRawPredictionCol(\"rpred2\")\n",
        "  .setMaxIter(10)\n",
        "  .setFeatureSubsetStrategy(\"auto\")\n",
        "\n",
        "val pred_pipeline = new Pipeline().setStages(Array(scaleAss, scaler, ass))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592162217733,
          "endTs" : 1592162217965
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val finalDf = pred_pipeline.fit(trainDf).transform(trainDf).select(\"features\")\n",
        "finalDf"
      ],
      "outputs" : [
        {
          "execution_count" : 10,
          "data" : {
            "text/plain" : [
              "[features: vector]"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "DataFrame"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592162219681,
          "endTs" : 1592162219822
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "trainDf.show(false)\n",
        "finalDf.show(false)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+------------+-------------+-----------------+------------------+-------+-------+-------+------+\n",
            "|value|text        |binaryFeature|arbraryIntFeature|arrayStringFeature|target1|target2|tweedId|userId|\n",
            "+-----+------------+-------------+-----------------+------------------+-------+-------+-------+------+\n",
            "|0    |Hello World.|1            |0                |[]                |1      |1      |0      |0     |\n",
            "|1    |Hello World.|0            |1                |[]                |1      |1      |1      |1     |\n",
            "|2    |Hello World.|0            |2                |[]                |0      |0      |2      |2     |\n",
            "|3    |Hello World.|1            |3                |[Tag1, Tag2]      |1      |1      |3      |3     |\n",
            "|4    |Other       |0            |4                |[]                |0      |1      |4      |4     |\n",
            "|5    |Hello World.|0            |5                |[]                |1      |1      |5      |5     |\n",
            "|6    |Hello World.|1            |6                |[]                |1      |1      |6      |6     |\n",
            "|7    |Hello World.|1            |7                |[Tag1, Tag2]      |1      |0      |7      |7     |\n",
            "|8    |Hello World.|1            |8                |[]                |0      |1      |8      |8     |\n",
            "|9    |Hello World.|0            |9                |[Tag1, Tag2]      |1      |0      |9      |9     |\n",
            "|10   |Hello World.|0            |10               |[]                |0      |1      |10     |10    |\n",
            "|11   |Hello World.|0            |11               |[Tag1, Tag2]      |0      |1      |11     |11    |\n",
            "|12   |Hello World.|1            |12               |[]                |1      |1      |12     |12    |\n",
            "|13   |Hello World.|0            |13               |[]                |1      |1      |13     |13    |\n",
            "|14   |Hello World.|1            |14               |[]                |1      |1      |14     |14    |\n",
            "|15   |Hello World.|0            |15               |[Tag1, Tag2]      |1      |0      |15     |15    |\n",
            "|16   |Hello World.|1            |16               |[Tag1, Tag2]      |1      |0      |16     |16    |\n",
            "|17   |Hello World.|0            |17               |[]                |1      |1      |17     |17    |\n",
            "|18   |Other       |0            |18               |[Tag1, Tag2]      |0      |0      |18     |18    |\n",
            "|19   |Hello World.|1            |19               |[]                |1      |0      |19     |19    |\n",
            "+-----+------------+-------------+-----------------+------------------+-------+-------+-------+------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+\n",
            "|features  |\n",
            "+----------+\n",
            "|[1.0,0.0] |\n",
            "|[0.0,1.0] |\n",
            "|[0.0,2.0] |\n",
            "|[1.0,3.0] |\n",
            "|[0.0,4.0] |\n",
            "|[0.0,5.0] |\n",
            "|[1.0,6.0] |\n",
            "|[1.0,7.0] |\n",
            "|[1.0,8.0] |\n",
            "|[0.0,9.0] |\n",
            "|[0.0,10.0]|\n",
            "|[0.0,11.0]|\n",
            "|[1.0,12.0]|\n",
            "|[0.0,13.0]|\n",
            "|[1.0,14.0]|\n",
            "|[0.0,15.0]|\n",
            "|[1.0,16.0]|\n",
            "|[0.0,17.0]|\n",
            "|[0.0,18.0]|\n",
            "|[1.0,19.0]|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 12,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592162232043,
          "endTs" : 1592162232726
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val vecToArray = udf( (xs: org.apache.spark.ml.linalg.DenseVector) => xs.toArray )\n",
        "val elements = Array(\"f1\", \"f2\")\n",
        "val sqlExpr = elements.zipWithIndex.map{ case (alias, idx) => col(\"feat_arr\").getItem(idx).as(alias) }\n",
        "\n",
        "finalDf.withColumn(\"feat_arr\", vecToArray(col(\"features\"))).select(sqlExpr : _*).show\n"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+---+----+\n",
            "| f1|  f2|\n",
            "+---+----+\n",
            "|1.0| 0.0|\n",
            "|0.0| 1.0|\n",
            "|0.0| 2.0|\n",
            "|1.0| 3.0|\n",
            "|0.0| 4.0|\n",
            "|0.0| 5.0|\n",
            "|1.0| 6.0|\n",
            "|1.0| 7.0|\n",
            "|1.0| 8.0|\n",
            "|0.0| 9.0|\n",
            "|0.0|10.0|\n",
            "|0.0|11.0|\n",
            "|1.0|12.0|\n",
            "|0.0|13.0|\n",
            "|1.0|14.0|\n",
            "|0.0|15.0|\n",
            "|1.0|16.0|\n",
            "|0.0|17.0|\n",
            "|0.0|18.0|\n",
            "|1.0|19.0|\n",
            "+---+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591974570155,
          "endTs" : 1591974570786
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "finalDf.select(\"target1\", \"prob1\", \"pred1\", \"features\").show(false)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 14,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591974570788,
          "endTs" : 1591974571319
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "finalDf.select(\"target2\", \"prob2\", \"pred2\", \"features\").show(false)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 15,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591974571321,
          "endTs" : 1591974571431
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "finalDf"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 16,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591975784878,
          "endTs" : 1591975785723
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// only get relevant columns\n",
        "val toArr: Any => Double = _.asInstanceOf[DenseVector].toArray(1)\n",
        "val toArrUdf = udf(toArr)\n",
        "\n",
        "finalDf.withColumn(\"output2\", toArrUdf('prob2)).select(\"userId\", \"tweedId\", \"output2\").show()"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 17,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591994167717,
          "endTs" : 1591994663361
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "text_trans_pipeline.save(dataDir + \"/pipeline_nlp.model\")\n",
        "pred_pipeline.save(dataDir + \"/pipeline_ml.model\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 18,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592025164354,
          "endTs" : 1592025164364
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "class VectorLookupModel(override val uid: String = \"VECTORLOOKUPKDSAFJKDSAJF\") extends Transformer {\n",
        "\n",
        "}\n",
        "\n",
        "class ArrayVectorAggregator(override val uid: String = \"ARRAYVECTORAGGREGATORKDSAFJKDSAJF\") extends Estimator[VectorLookupModel] {\n",
        "\n",
        "  // Transformer Params\n",
        "  // Defining a Param requires 3 elements:\n",
        "  //  - Param definVeition\n",
        "  //  - Param getter method\n",
        "  //  - Param setter method\n",
        "  // (The getter and setter are technically not required, but they are nice standards to follow.)\n",
        "\n",
        "  /**\n",
        "   * Param for input array column name.\n",
        "   */\n",
        "  final val inputCol: Param[String] = new Param[String](this, \"inputCol\", \"input column name\")\n",
        "\n",
        "  final def getInputCol: String = $(inputCol)\n",
        "\n",
        "  final def setInputCol(value: String): VectorLookupModel = set(inputCol, value)\n",
        "\n",
        "  /**\n",
        "   * Param for lookup vector column name.\n",
        "   */\n",
        "  final val vectorCol: Param[String] = new Param[String](this, \"outputCol\", \"output column name\")\n",
        "\n",
        "  final def getVectorCol: String = $(outputCol)\n",
        "\n",
        "  final def setVectorCol(value: String): VectorLookupModel = set(outputCol, value)\n",
        "\n",
        "  /**\n",
        "   * Param for output column name.\n",
        "   */\n",
        "  final val outputCol: Param[String] = new Param[String](this, \"outputCol\", \"output column name\")\n",
        "\n",
        "  final def getOutputCol: String = $(outputCol)\n",
        "\n",
        "  final def setOutputCol(value: String): VectorLookupModel = set(outputCol, value)\n",
        "\n",
        "  /**\n",
        "   * This method implements the main transformation.\n",
        "   * Its required semantics are fully defined by the method API: take a Dataset or DataFrame,\n",
        "   * and return a DataFrame.\n",
        "   *\n",
        "   * Most Transformers are 1-to-1 row mappings which add one or more new columns and do not\n",
        "   * remove any columns.  However, this restriction is not required.  This example does a flatMap,\n",
        "   * so we could either (a) drop other columns or (b) keep other columns, making copies of values\n",
        "   * in each row as it expands to multiple rows in the flatMap.  We do (a) for simplicity.\n",
        "   */\n",
        "  override def fit(dataset: Dataset[_]): VectorLookupModel = {\n",
        "    // dataset.withColumn($(outputCol), explode(col($(inputCol))))\n",
        "  }\n",
        "\n",
        "  /**\n",
        "   * Check transform validity and derive the output schema from the input schema.\n",
        "   *\n",
        "   * We check validity for interactions between parameters during `transformSchema` and\n",
        "   * raise an exception if any parameter value is invalid. Parameter value checks which\n",
        "   * do not depend on other parameters are handled by `Param.validate()`.\n",
        "   *\n",
        "   * Typical implementation should first conduct verification on schema change and parameter\n",
        "   * validity, including complex parameter interaction checks.\n",
        "   */\n",
        "  override def transformSchema(schema: StructType): StructType = {\n",
        "      val inputColType = schema.fields(schema.fieldIndex($(inputCol))).dataType.asInstanceOf[ArrayType];\n",
        "      // Compute output type.\n",
        "      // This is important to do correctly when plugging this Transformer into a Pipeline,\n",
        "      // where downstream Pipeline stages may expect use this Transformer's output as their input.\n",
        "    //   schema.add($(outputCol), inputColType.elementType)\n",
        "  }\n",
        "\n",
        "  /**\n",
        "   * Creates a copy of this instance.\n",
        "   * Requirements:\n",
        "   *  - The copy must have the same UID.\n",
        "   *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n",
        "   *    argument.\n",
        "   *  - This should do a deep copy of any data members which are mutable.  That said,\n",
        "   *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n",
        "   *    method often suffices.\n",
        "   * @param extra  Param values which will overwrite Params in the copy.\n",
        "   */\n",
        "  override def copy(extra: ParamMap): Estimator = defaultCopy(extra)\n",
        "}\n"
      ],
      "outputs" : [
      ]
    }
  ]
}