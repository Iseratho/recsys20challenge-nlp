{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2",
          "com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.2"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "8g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "14g",
        "spark.local.dir" : "/var/cache/spark"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# NLP RECSYS Run Pipeline Stage1\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1592174328909,
          "endTs" : 1592174329473
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer}\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "\n",
        "val dataDir = sys.env(\"HOME\") + \"/recsys2020\""
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592174329489,
          "endTs" : 1592174329751
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "text/html" : [
              "\n",
              "<div class=\"object-display spark-ui\">\n",
              "  <span class=\"field-name\">Spark UI</span><a href=\"http://127.0.0.1:4040\" class=\"link\" target=\"_blank\">http://127.0.0.1:4040</a>\n",
              "</div>\n",
              "\n",
              "<details class=\"object-display\">\n",
              "  <summary class=\"object-summary\"><span class=\"summary-content\"><span>SparkConf</span></span></summary>\n",
              "  <ul class=\"object-fields\">\n",
              "          \n",
              "<li>\n",
              "<span class=\"field-name\">spark.driver.host</span><span class=\"string\">127.0.0.1</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.driver.port</span><span class=\"string\">35105</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.repl.class.uri</span><span class=\"string\">spark://127.0.0.1:35105/classes</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.jars</span><span class=\"string\">/home/sko/tools/polynote/deps/polynote-spark-runtime.jar,/home/sko/tools/polynote/deps/polynote-spark-runtime.jar,/home/sko/tools/polynote/deps/polynote-spark-runtime.jar,https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar,https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp-gpu_2.11/2.5.2/spark-nlp-gpu_2.11-2.5.2.jar,https://repo1.maven.org/maven2/net/sf/trove4j/trove4j/3.0.3/trove4j-3.0.3.jar,https://repo1.maven.org/maven2/org/tensorflow/tensorflow/1.15.0/tensorflow-1.15.0.jar,https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.9/httpclient-4.5.9.jar,https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.11.603/aws-java-sdk-core-1.11.603.jar,https://repo1.maven.org/maven2/com/navigamez/greex/1.0/greex-1.0.jar,https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-kms/1.11.603/aws-java-sdk-kms-1.11.603.jar,https://repo1.maven.org/maven2/com/amazonaws/jmespath-java/1.11.603/jmespath-java-1.11.603.jar,https://repo1.maven.org/maven2/com/github/universal-automata/liblevenshtein/3.0.0/liblevenshtein-3.0.0.jar,https://repo1.maven.org/maven2/com/typesafe/config/1.3.0/config-1.3.0.jar,https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.11/httpcore-4.4.11.jar,https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.11/2.5.2/spark-nlp_2.11-2.5.2.jar,https://repo1.maven.org/maven2/org/tensorflow/libtensorflow/1.15.0/libtensorflow-1.15.0.jar,https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar,https://repo1.maven.org/maven2/joda-time/joda-time/2.9.5/joda-time-2.9.5.jar,https://repo1.maven.org/maven2/org/joda/joda-convert/1.8.1/joda-convert-1.8.1.jar,https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.7.2/jackson-databind-2.6.7.2.jar,https://repo1.maven.org/maven2/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar,https://repo1.maven.org/maven2/org/tensorflow/libtensorflow_jni/1.15.0/libtensorflow_jni-1.15.0.jar,https://repo1.maven.org/maven2/org/json4s/json4s-ext_2.11/3.5.3/json4s-ext_2.11-3.5.3.jar,https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar,https://repo1.maven.org/maven2/org/tensorflow/libtensorflow_jni_gpu/1.15.0/libtensorflow_jni_gpu-1.15.0.jar,https://repo1.maven.org/maven2/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar,https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar,https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar,https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.6.7/jackson-dataformat-cbor-2.6.7.jar,https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.11.603/aws-java-sdk-s3-1.11.603.jar,https://repo1.maven.org/maven2/org/rocksdb/rocksdbjni/6.5.3/rocksdbjni-6.5.3.jar</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.repl.class.outputDir</span><span class=\"string\">/home/sko/.cache/spark/spark-0013514a-8d4f-4363-bdf7-dbed1c96ffd9/repl-c1a5ed2e-fb33-4ab5-b6f6-23529c9e5ef1</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.app.name</span><span class=\"string\">Polynote 0.3.10: recsys/run_pipeline_stage1.ipynb</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.memory.offHeap.enabled</span><span class=\"string\">true</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.driver.memory</span><span class=\"string\">8g</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.memory.offHeap.size</span><span class=\"string\">32g</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.executor.id</span><span class=\"string\">driver</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.driver.extraJavaOptions</span><span class=\"string\">-Dlog4j.configuration=log4j.properties -Djava.library.path=/home/sko/.local/lib/python3.8/site-packages/jep</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.submit.deployMode</span><span class=\"string\">client</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.master</span><span class=\"string\">local[*]</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.executor.memory</span><span class=\"string\">14g</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.local.dir</span><span class=\"string\">/var/cache/spark</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.home</span><span class=\"string\">/opt/apache-spark</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.sql.catalogImplementation</span><span class=\"string\">hive</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.repl.local.jars</span><span class=\"string\">file:///home/sko/tools/polynote/deps/polynote-spark-runtime.jar,file:///home/sko/tools/polynote/deps/polynote-runtime.jar</span>\n",
              "</li>\n",
              "            \n",
              "<li>\n",
              "<span class=\"field-name\">spark.app.id</span><span class=\"string\">local-1592174324898</span>\n",
              "</li>\n",
              "            \n",
              "\n",
              "</ul></details>\n",
              "          \n",
              "           "
            ],
            "text/plain" : [
              "org.apache.spark.sql.SparkSession@202222d4"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "SparkSession"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592174329758,
          "endTs" : 1592174330244
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "package nlprecsys\n",
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer}\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "\n",
        "class Exploder(override val uid: String) extends Transformer with DefaultParamsWritable {\n",
        "  def this() = this(Identifiable.randomUID(\"Exploder\"))\n",
        "  def setInputCol(value: String): this.type = set(inputCol, value)\n",
        "  def setOutputCol(value: String): this.type = set(outputCol, value)\n",
        "  def getOutputCol: String = getOrDefault(outputCol)\n",
        "  val inputCol = new Param[String](this, \"inputCol\", \"input column\")\n",
        "  val outputCol = new Param[String](this, \"outputCol\", \"output column\")\n",
        "\n",
        "  override def transform(dataset: Dataset[_]): DataFrame = {\n",
        "    val outCol = extractParamMap.getOrElse(outputCol, \"output\")\n",
        "    val inCol = extractParamMap.getOrElse(inputCol, \"input\")\n",
        "    dataset.withColumn(outCol, explode(col(inCol)))\n",
        "  }\n",
        "\n",
        "  override def transformSchema(schema: StructType): StructType = {\n",
        "      val outCol = extractParamMap.getOrElse(outputCol, \"output\")\n",
        "      val inCol = extractParamMap.getOrElse(inputCol, \"input\")\n",
        "      val inputColType = schema.fields(schema.fieldIndex(inCol)).dataType.asInstanceOf[ArrayType];\n",
        "      schema.add(outCol, inputColType.elementType)\n",
        "  }\n",
        "  override def copy(extra: ParamMap): Transformer = defaultCopy(extra)\n",
        "}\n",
        "object Exploder extends DefaultParamsReadable[Exploder] {\n",
        "  override def load(path: String): Exploder = super.load(path)\n",
        "}\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592174330248,
          "endTs" : 1592174768352
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val pipeline = PipelineModel.load(dataDir + \"/pipeline_stage1_v1\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592176609998,
          "endTs" : 1592176611433
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val dsName = \"training1m\"\n",
        "\n",
        "val classNames = Array(\n",
        "  \"retweet\",\n",
        "  \"retweet_with_comment\",\n",
        "  \"like\",\n",
        "  \"reply\")\n",
        "val labelColumns = for (className <- classNames) yield \"has_\" + className;\n",
        "val idCols = Array(\"tweet_id\", \"user_id\")\n",
        "val df = spark.read.parquet(dataDir + s\"/${dsName}.parquet\").limit(100000)\n",
        "val relevantCols = idCols ++  Array(\"features\") ++ labelColumns\n",
        "val transDf = pipeline.transform(df).selectExpr(relevantCols:_*)\n",
        "\n",
        "// this fails with out of disk space because the data is too big\n",
        "// transDf\n",
        "//     .write\n",
        "//     .mode(SaveMode.Overwrite)\n",
        "//     .parquet(dataDir + s\"/${dsName}_stage1.parquet\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592176613019,
          "endTs" : 1592176865634
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// convert to float columns to make data smaller\n",
        "val vecToArray = udf( (xs: org.apache.spark.ml.linalg.DenseVector) => xs.toArray )\n",
        "val elements = for (i <- 1 to 522) yield \"f_\" + i\n",
        "\n",
        "val sqlExpr = idCols.map{x => col(x)} ++ \n",
        "              elements.zipWithIndex.map{ case (alias, idx) => col(\"feat_arr\").getItem(idx).cast(FloatType).as(alias) } ++\n",
        "              labelColumns.map{x => col(x).cast(BooleanType)}\n",
        "\n",
        "transDf.withColumn(\"feat_arr\", vecToArray(col(\"features\"))).select(sqlExpr : _*)\n",
        "    .write\n",
        "    .mode(SaveMode.Overwrite)\n",
        "    .parquet(dataDir + s\"/${dsName}_stage1.parquet\")"
      ],
      "outputs" : [
      ]
    }
  ]
}