{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "20g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "20g",
        "spark.local.dir" : "/home/sko/.cache/spark"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# NLP RECSYS Train Pipeline Stage1\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592077118633,
          "endTs" : 1592077118812
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer}\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "\n",
        "\n",
        "val dataDir = sys.env(\"HOME\") + \"/recsys2020\"\n",
        "val dsName = \"training\""
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592077118819,
          "endTs" : 1592077119102
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = spark.read.parquet(dataDir + s\"/${dsName}.parquet\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592077119106,
          "endTs" : 1592077119266
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "package nlprecsys\n",
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import org.apache.spark.ml.{Pipeline, PipelineModel, Transformer}\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.storage._\n",
        "import org.apache.spark.ml.feature._\n",
        "import org.apache.spark.ml.classification._\n",
        "import org.apache.spark.ml.linalg.DenseVector\n",
        "import org.apache.spark.ml.param.{Param, ParamMap}\n",
        "import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\n",
        "import org.apache.spark.sql.{DataFrame, Dataset}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.functions.{col, explode, udf}\n",
        "import org.apache.spark.sql.types.{DataTypes, StructType}\n",
        "\n",
        "class Exploder(override val uid: String) extends Transformer with DefaultParamsWritable {\n",
        "  def this() = this(Identifiable.randomUID(\"Exploder\"))\n",
        "  def setInputCol(value: String): this.type = set(inputCol, value)\n",
        "  def setOutputCol(value: String): this.type = set(outputCol, value)\n",
        "  def getOutputCol: String = getOrDefault(outputCol)\n",
        "  val inputCol = new Param[String](this, \"inputCol\", \"input column\")\n",
        "  val outputCol = new Param[String](this, \"outputCol\", \"output column\")\n",
        "\n",
        "  override def transform(dataset: Dataset[_]): DataFrame = {\n",
        "    val outCol = extractParamMap.getOrElse(outputCol, \"output\")\n",
        "    val inCol = extractParamMap.getOrElse(inputCol, \"input\")\n",
        "    dataset.withColumn(outCol, explode(col(inCol)))\n",
        "  }\n",
        "\n",
        "  override def transformSchema(schema: StructType): StructType = {\n",
        "      val outCol = extractParamMap.getOrElse(outputCol, \"output\")\n",
        "      val inCol = extractParamMap.getOrElse(inputCol, \"input\")\n",
        "      val inputColType = schema.fields(schema.fieldIndex(inCol)).dataType.asInstanceOf[ArrayType];\n",
        "      schema.add(outCol, inputColType.elementType)\n",
        "  }\n",
        "  override def copy(extra: ParamMap): Transformer = defaultCopy(extra)\n",
        "}\n",
        "object Exploder extends DefaultParamsReadable[Exploder] {\n",
        "  override def load(path: String): Exploder = super.load(path)\n",
        "}\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592077119269,
          "endTs" : 1592077120744
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val doc = new DocumentAssembler()\n",
        "    .setInputCol(\"tweet_text\")\n",
        "    .setOutputCol(\"document\")\n",
        "    .setCleanupMode(\"shrink\")\n",
        "\n",
        "val use = UniversalSentenceEncoder\n",
        "      .pretrained()\n",
        "      .setInputCols(Array(\"document\"))\n",
        "      .setOutputCol(\"tweet_embeddings\")\n",
        "\n",
        "val fin = new EmbeddingsFinisher()\n",
        "      .setInputCols(use.getOutputCol)\n",
        "      .setOutputCols(\"finished_tweet_embeddings\")\n",
        "      .setOutputAsVector(true)\n",
        "      .setCleanAnnotations(false)\n",
        "\n",
        "val exploder = new Exploder()\n",
        "  .setInputCol(fin.getOutputCols(0))\n",
        "  .setOutputCol(\"embedding_features\")\n",
        "\n",
        "val tweetTypeIndexer = new StringIndexerModel(Array(\"TopLevel\", \"Retweet\", \"Quote\", \"Reply\"))\n",
        "  .setInputCol(\"tweet_type\")\n",
        "  .setOutputCol(\"tweet_type_idx\");\n",
        "\n",
        "val tweetTypeEncoder = new OneHotEncoder()\n",
        "  .setInputCol(tweetTypeIndexer.getOutputCol)\n",
        "  .setOutputCol(\"tweet_type_onehot\")\n",
        "\n",
        "val scaleAss = new VectorAssembler()\n",
        "  .setInputCols(Array(\"author_follower_count\", \"author_following_count\", \"user_follower_count\", \"user_following_count\"))\n",
        "  .setOutputCol(\"count_features\")\n",
        "\n",
        "val scaler = new StandardScaler()\n",
        "  .setInputCol(scaleAss.getOutputCol)\n",
        "  .setOutputCol(\"count_features_scaled\")\n",
        "  .setWithStd(true)\n",
        "  .setWithMean(false)\n",
        "\n",
        "val ass = new VectorAssembler()\n",
        "  .setInputCols(Array(\n",
        "      \"embedding_features\",\n",
        "      tweetTypeEncoder.getOutputCol,\n",
        "      scaler.getOutputCol,\n",
        "      \"author_is_verified\",\n",
        "      \"user_is_verified\",\n",
        "      \"follows\"\n",
        "    )).setOutputCol(\"features\")\n",
        "\n",
        "val classNames = Array(\n",
        "  \"retweet\",\n",
        "  \"retweet_with_comment\",\n",
        "  \"like\",\n",
        "  \"reply\")\n",
        "\n",
        "val pipeline_stage1 = new Pipeline().setStages(Array(\n",
        "  scaleAss, \n",
        "  scaler, \n",
        "  tweetTypeIndexer, \n",
        "  tweetTypeEncoder, \n",
        "  doc, \n",
        "  use,\n",
        "  fin,\n",
        "  exploder,\n",
        "  ass))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592077120751,
          "endTs" : 1592077652733
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val fitted_pipeline_stage1 = pipeline_stage1.fit(df)\n",
        "fitted_pipeline_stage1.write.overwrite().save(dataDir + \"/pipeline_stage1_v1\")"
      ],
      "outputs" : [
      ]
    }
  ]
}