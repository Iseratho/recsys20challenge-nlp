{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "12g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "2g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# NLP RECSYS dataset subsampling\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592249102249,
          "endTs" : 1592249102559
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import com.johnsnowlabs.ml.tensorflow.TensorflowBert\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "\n",
        "val dataDir = sys.env(\"HOME\") + \"/recsys2020\"\n",
        "val inDsName = \"training\"\n",
        "val outDsName = \"training1m_val\"\n",
        "val samples = 1000"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592249102565,
          "endTs" : 1592249163077
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// val df = spark.read.parquet(dataDir + \"/training1m.parquet\")\n",
        "val df = spark.read.format(\"csv\")\n",
        "        .option(\"delimiter\", \"\\u0001\")\n",
        "        .load(dataDir + s\"/${inDsName}.tsv\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592249482713,
          "endTs" : 1592251429545
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sampled_df = df.sample(samples * 1.0 / df.count())\n",
        "// sampled_df.write.mode(SaveMode.Overwrite).parquet(dataDir + \"/training1m_val.parquet\")\n",
        "sampled_df.write.format(\"csv\")\n",
        "        .option(\"delimiter\", \"\\u0001\")\n",
        "        .save(dataDir + s\"/${outDsName}.tsv\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1592249163154,
          "endTs" : 1592249163350
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}