{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0"
        ]
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.memory.offHeap.enabled" : "true",
        "spark.driver.memory" : "12g",
        "spark.memory.offHeap.size" : "32g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "2g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# sentence encoder\n",
        "\n",
        "\n",
        "Based on [https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32](https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591902674599,
          "endTs" : 1591902675402
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import com.johnsnowlabs.nlp.SparkNLP\n",
        "import com.johnsnowlabs.nlp.annotator._\n",
        "import com.johnsnowlabs.nlp.base._\n",
        "import com.johnsnowlabs.ml.tensorflow.TensorflowBert\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import org.apache.spark.sql.functions.{udf,to_timestamp}\n",
        "import org.apache.spark.ml.feature.QuantileDiscretizer\n",
        "\n",
        "val dataDir = \"../../data/recsys2020\""
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591902675417,
          "endTs" : 1591902701444
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "var df = spark.read.parquet(dataDir + \"/training1m.parquet\")\n",
        "df"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "text/plain" : [
              "[text_tokens: array<string>, hashtags: array<string> ... 22 more fields]"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "DataFrame"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591902701454,
          "endTs" : 1591902707228
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// Convert targets\n",
        "val udf_has_engagement = udf[Boolean, IntegerType](_ != null)\n",
        "val target_df = df.withColumn(\"has_retweet\", udf_has_engagement('retweet_timestamp))\n",
        "target_df.head()"
      ],
      "outputs" : [
        {
          "execution_count" : 9,
          "data" : {
            "text/plain" : [
              "[WrappedArray([CLS], RT, @, F, ##bro, ##glia, :, Después, de, analiza, ##r, ex, ##haus, ##tiva, ##mente, el, mercado, con, ##clu, ##yo, que, los, biz, ##co, ##chi, ##tos, ale, ##m, de, que, ##so, son, lo, mejor, q, nos, dio, entre, ríos, h, [UNK], [SEP]),null,FB6304C97F6CC05AF0ADBBCD328CE572,null,null,null,Retweet,06D61DCBBE938971E1EA0C38BD9B5446,1581420802,42276CF9E61425149BD30B3312C76E20,610,215,false,1281271512,D0EA9DDFE93EDA782D94184BDA6F0A41,841,270,false,1362976270,false,null,null,null,1581428582,false]"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Row"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591902707240,
          "endTs" : 1591902707486
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// reduce workload while experimenting\n",
        "df = df.limit(100)\n",
        "df"
      ],
      "outputs" : [
        {
          "execution_count" : 7,
          "data" : {
            "text/plain" : [
              "[text_tokens: array<string>, hashtags: array<string> ... 22 more fields]"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "DataFrame"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591902403293
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val text_df = df.withColumn(\"sample_text\", lit(\"Hello World.\"))\n",
        "text_df.head()"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591893859121,
          "endTs" : 1591893859719
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val udf_tweet = udf[String, Array[String]](_.mkString(\" \"))\n",
        "\n",
        "// filtering\n",
        "// .filterNot(token => List(\"[CLS]\",\"[UNK]\",\"[SEP]\",\"UNKN\").contains(token))\n",
        "// folding instead of mkstring\n",
        "// .foldLeft(token => )\n",
        "val converted_df = df.withColumn(\"sentence\", udf_tweet('text_tokens))\n",
        "converted_df.head()"
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.SparkException",
          "evalue" : "Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (array<string>) => string)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to [Ljava.lang.String;\n\tat notebook0.Cell8$3$$anonfun$1.apply(Cell8:1)\n\t... 21 more\n\nDriver stacktrace:",
          "traceback" : [
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591889795781,
          "endTs" : 1591890239359
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "// Define stages\n",
        "val use = UniversalSentenceEncoder.pretrained().setInputCols(Array(\"text_tokens\")).setOutputCol(\"tweet_embeddings\")\n",
        "val fin = new EmbeddingsFinisher()\n",
        "      .setInputCols(\"tweet_embeddings\")\n",
        "      .setOutputCols(\"finished_tweet_embeddings\")\n",
        "      .setOutputAsVector(true)\n",
        "      .setCleanAnnotations(false)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "Download done! Loading the resource.\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591890239364,
          "endTs" : 1591890239569
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val sent_emb = new SentenceEmbeddings().setInputCols(Array(\"text_tokens\"))"
      ],
      "outputs" : [
        {
          "ename" : "java.lang.IllegalArgumentException",
          "evalue" : "requirement failed: setInputCols in SENTENCE_EMBEDDINGS_a0b123625945 expecting 2 columns. Provided column amount: 1. Which should be columns from the following annotators: document, word_embeddings",
          "traceback" : [
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591890239572,
          "endTs" : 1591890240158
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val use_clf_pipeline = new Pipeline().setStages(Array(use, fin))"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1591890240164,
          "endTs" : 1591890240301
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val use_transformed = use_clf_pipeline.fit(df).transform(df)\n",
        "use_transformed"
      ],
      "outputs" : [
        {
          "ename" : "java.lang.IllegalArgumentException",
          "evalue" : "requirement failed: Wrong or missing inputCols annotators in UNIVERSAL_SENTENCE_ENCODER_4de71669b7ec.\n\nCurrent inputCols: text_tokens. Dataset's columns:\n(column_name=text_tokens,is_nlp_annotator=false)\n(column_name=hashtags,is_nlp_annotator=false)\n(column_name=tweet_id,is_nlp_annotator=false)\n(column_name=present_media,is_nlp_annotator=false)\n(column_name=present_links,is_nlp_annotator=false)\n(column_name=present_domains,is_nlp_annotator=false)\n(column_name=tweet_type,is_nlp_annotator=false)\n(column_name=language,is_nlp_annotator=false)\n(column_name=tweet_timestamp,is_nlp_annotator=false)\n(column_name=engaged_with_user_id,is_nlp_annotator=false)\n(column_name=engaged_with_user_follower_count,is_nlp_annotator=false)\n(column_name=engaged_with_user_following_count,is_nlp_annotator=false)\n(column_name=engaged_with_user_is_verified,is_nlp_annotator=false)\n(column_name=engaged_with_user_account_creation,is_nlp_annotator=false)\n(column_name=engaging_user_id,is_nlp_annotator=false)\n(column_name=engaging_user_follower_count,is_nlp_annotator=false)\n(column_name=engaging_user_following_count,is_nlp_annotator=false)\n(column_name=engaging_user_is_verified,is_nlp_annotator=false)\n(column_name=engaging_user_account_creation,is_nlp_annotator=false)\n(column_name=engagee_follows_engager,is_nlp_annotator=false)\n(column_name=reply_timestamp,is_nlp_annotator=false)\n(column_name=retweet_timestamp,is_nlp_annotator=false)\n(column_name=retweet_with_comment_timestamp,is_nlp_annotator=false)\n(column_name=like_timestamp,is_nlp_annotator=false).\nMake sure such annotators exist in your pipeline, with the right output names and that they have following annotator types: document",
          "traceback" : [
          ],
          "output_type" : "error"
        }
      ]
    }
  ]
}